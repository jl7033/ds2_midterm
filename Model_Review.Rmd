---
title: "Model_Review"
author: "Brooklynn McNeil"
date: "2025-03-25"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(corrplot)
library(caret)
library(glmnet)
library(splines)
```

# Loading Data and Exploratory Data Analysis

When building models it is important to start with tidy data, removing any columns that won't be used as predictors (i.e "ID"). Make sure that categorical variables are read in as factors. 

## Upload Data (still need a relative path for this)

```{r}
load("./dat1.RData")
load("./dat2.RData")
```

## Specify Training and Testing Sets

When building a model we must partition the data into two sets: training and testing. All of the model building procedures will be done based on the training dataset including parameter optimization and model selection. Only the final model will be tested using the testing dataset to evaluate the performance and generalizability of the model. Generally, a data split will occur with 80% of the original data in the training set and the other 20% in the testing dataset.

```{r}
train_df = dat1 |> dplyr::select(-id)
test_df = dat2 |> dplyr::select(-id)
```

The data above has already been split, but we can use the caret package??? to create our partition.Always set a seed when doing this to control the randomness. Use this same seed for training all of the models.

```{r message=FALSE, warning=FALSE}
set.seed(56)
#using dat 1 as example data as if it is the only data we obtained

#datSplit = initial_split(data = dat1, prop = 0.8)
#trainData = training(datSplit)
#testData = testing(datSplit)
```

## Exploratory Data Analysis

Check the distribution of the response variable. For linear regression this needs to be normally distributed. Explore transformations such as log if it is not normally distributed. If using a generalized linear model the response variable doesn't necessarily need to be normally distributed.

```{r Plot Distribution}
train_df |>
  ggplot(aes(x = log_antibody)) +
  geom_histogram() +
  ggtitle("Distribution of Response Variable")
```
Check if there are any highly correlated predictors included. If there are, these should be watched throughout the model building processed and make sure only the most significant are removed.

```{r Evaluate Correlation}
# matrix of predictors
x = model.matrix(log_antibody ~ ., train_df) [,-1]

# correlation plot
corrplot(cor(x), method = "circle", type = "full")
```
We see that height, weight, and BMI have slight correlation which makes sense, we probably don't want all of these to be included in the final model. Systolic blood pressure (SBP) and hypertentsion are also correlated because hypertension means there is high SBP. 

# Model Building Process

Using the caret package, we can use the basic formula below, and change the `method` and some parameters for the type of model that we want to build. For this part we will use the simple linear model built using Ordinary Least Squares (OLS).

## Cross Validation

Before we actually start building the model we need to make sure we are doing cross validation. Use the `trainControl` function from caret to set up how we do cross validation.Below we set up a cross validation parameter with k=10 folds.

```{r CV}
# k-fold cv
ctrl1 = trainControl(method = "cv", number = 10)

# if you want to use the 1se rule
ctrl_1se = trainControl(method = "cv", number = 10, selectionFunction = "oneSE")

# leave-one-out CV
ctrl_2 = trainControl(method = "LOOCV")

# leave-group-out / Monte Carlo CV
ctrl_3 = trainControl(method = "LGOCV", p = 0.75, number = 50)

# bootstrap
ctrl_4 = trainControl(method = "boot632", number = 100)

# repeated K-fold CV
ctrl_5 = trainControl(method = "repeatedcv", repeats = 5, number = 10)

# only fit one model to the entire training set
ctrl_6 = trainControl(method = "none")
```

## Train the Model

Train the model using caret's `train` function and the `ctrl1` training parameters. For OLS use method "lm".
```{r OLS}
set.seed(56)
lm_fit = train(log_antibody ~ .,
               data = train_df,
               method = "lm",
               trControl = ctrl1)
  
# get the coefficients
summary(lm_fit)

```

Get the prediction error using predictions from the model and the actual response value from the test data. Take the mean squared error "MSE" to report the test error.

```{r Test Error}
MSPE_lm_pred = predict(lm_fit, newdata = test_df)

MSPE_lm = mean((MSPE_lm_pred - test_df$log_antibody)^2)
MSPE_lm
```
# Linear Models with Regularization

### Ridge Model

Ridge models have type 1 pentaly terms that will shrink the coefficients of highly correlated predictors, but not eliminate them from the model. To build a ridge model we set alpha to 0 and create a grid for lambda values that determine how large the penalty term can be.

```{r Ridge}
set.seed(56)
ridge_fit = train(log_antibody ~ .,
               data = train_df,
               method = "glmnet",
               trControl = ctrl1,
               tuneGrid = expand.grid(alpha = 0, # for ridge specifically
                                      lambda = exp(seq(-10,0, length = 100)))) # create gride for 100 different lambda values between -10, 0

# find the lambda value with lowest RMSE
plot(ridge_fit, xTrans = log)
ridge_fit$bestTune$lambda

MSPE_ridge_pred = predict(ridge_fit, newdata = test_df)
MSPE_ridge = mean((MSPE_ridge_pred - test_df$log_antibody)^2)
MSPE_ridge

```
### Lasso Model

Lasso models use type 2 penalty terms that ends up shrinking correlated or insignificant predictors to zero and remove them from the model. We use an alpha value of 0 instead of 1 to train the model as Lasso.

```{r Lasso}
set.seed(56)
lasso_fit = train(log_antibody ~ .,
               data = train_df,
               method = "glmnet",
               trControl = ctrl1,
               tuneGrid = expand.grid(alpha = 1, # for Lasso specifically
                                      lambda = exp(seq(-10,0, length = 100)))) # create gride for 100 different lambda values between -10, 0

# find the lambda value with lowest RMSE
plot(lasso_fit, xTrans = log)
lasso_fit$bestTune$lambda

MSPE_lasso_pred = predict(lasso_fit, newdata = test_df)
MSPE_lasso = mean((MSPE_lasso_pred - test_df$log_antibody)^2)
MSPE_lasso

```
### Elastic Net

Elastic Net models use a mixture of lasso and ridge penalty terms. We create a grid of alpha values to determine the best mixture of models.

```{r}
set.seed(56)
enet_fit = train(log_antibody ~ .,
               data = train_df,
               method = "glmnet",
               trControl = ctrl1,
               tuneGrid = expand.grid(alpha = seq(0,1, length = 21), # create a model for each alpha value with 0.5 spacing
                                      lambda = exp(seq(-10,0, length = 100)))) # create gride for 100 different lambda values between -10, 0

# find the lambda value with lowest RMSE
myCol = rainbow(25)
myPar = list(superpose.symbol = list(col = myCol),
             superpose.line = list(col = myCol))

plot(enet_fit, par.settings = myPar, xTrans = log)
enet_fit$bestTune$lambda

MSPE_enet_pred = predict(enet_fit, newdata = test_df)
MSPE_enet = mean((MSPE_enet_pred - test_df$log_antibody)^2)
MSPE_enet

```
# Dimension Reduction Methods

### Principal Components Regression (PCR)

PCR performs a principal component analysis on the predictors to reduce the dimensionality of the data. It then regresses the response variable on the principal components (PCs) instead of the original predictors. This type of model can work well with data that is highly correlated. This method is unsupervised and doesn't consider the response variable when selecting components.

We need to specify the number of components for the model to consider. We use the number of predictors in the model matrix without the response variable. The data must be centered and scaled so that the components are weighed equally.

```{r}

set.seed(56)

# get number of components to include
ncol(x) # x is model matrix of predictors

# Build PCR model
pcr_fit = train(log_antibody ~ .,
               data = train_df,
               method = "pcr",
               trControl = ctrl1,
               tuneLength = 15,
               preProcess = c("center", "scale")) 

MSPE_pcr_pred = predict(pcr_fit, newdata = test_df)
MSPE_pcr = mean((MSPE_pcr_pred - test_df$log_antibody)^2)
MSPE_pcr

```

### Partial Least Square Regression (PLS)

PLS is similar to PCR, but it is a supervised approach. It finds components that explain both the predictor variance and the response correlation. It is more likely to retain predictive signal than PCR, but can overfit if too many components are used.

```{r}

set.seed(56)

pls_fit = train(log_antibody ~ .,
               data = train_df,
               method = "pls",
               trControl = ctrl1,
               tuneLength = 15,
               preProcess = c("center", "scale")) 

MSPE_pls_pred = predict(pls_fit, newdata = test_df)
MSPE_pls = mean((MSPE_pls_pred - test_df$log_antibody)^2)
MSPE_pls

```

## GAM/MARS

### Generalized Additive Models

```{r}

set.seed(56)

gam_fit = train(log_antibody ~ .,
               data = train_df,
               method = "gam",
               trControl = ctrl1)
summary(gam_fit)

MSPE_gam_pred = predict(gam_fit, newdata = test_df)
MSPE_gam = mean((MSPE_gam_pred - test_df$log_antibody)^2)
MSPE_gam

```

###

### Multivariate Additive Regression (MARS)

```{r}
set.seed(56)

mars_grid <- expand.grid(degree = 1:3,nprune = 2:15)

mars_fit = train(log_antibody ~ .,
               data = train_df,
               method = "earth",
               tuneGrid = mars_grid,
               trControl = ctrl1)
ggplot(mars_fit)

MSPE_mars_pred = predict(mars_fit, newdata = test_df)
MSPE_mars = mean((MSPE_mars_pred - test_df$log_antibody)^2)
MSPE_mars

```

### PDPs

```{r}

p1 <- pdp::partial(mars_fit, pred.var = c("bmi"), grid.resolution = 10) |> autoplot()

p2 <- pdp::partial(mars_fit, pred.var = c("bmi", "time"),
grid.resolution = 10) |>
pdp::plotPartial(levelplot = FALSE, zlab = "yhat", drape = TRUE,
screen = list(z = 20, x = -60))
gridExtra::grid.arrange(p1, p2, ncol = 2)

```


# Comparing Different Models

```{r}
resamp = resamples(list(
  lm = lm_fit,
  enet = enet_fit,
  lasso = lasso_fit,
  ridge = ridge_fit,
  pcr = pcr_fit,
  pls = pls_fit,
  gam = gam_fit,
  mars = mars_fit
))

parallelplot(resamp, metric = "RMSE")

bwplot(resamp, metric = "RMSE")
```
# Nonlinear methods

We need to split the data to x and y to easily use the nonlinear methods

```{r}
x = model.matrix(log_antibody ~ ., train_df) [,-1]

y = train_df$log_antibody
```

## Univarite Models

### Polynomial Regression

Polynomial regression models fit the data with the addition of squared and cubed predictor terms as well as interactions. These models can help fit the complexity of the data, but are notoriously bad for prediction outside of the observation range.

We can test what order polynomial is best.

```{r}
poly_fit1 = lm(log_antibody ~ SBP,
              data = train_df)
poly_fit2 = lm(log_antibody ~ poly(SBP, 2),
              data = train_df)
poly_fit3 = lm(log_antibody ~ poly(SBP, 3),
              data = train_df)
poly_fit4 = lm(log_antibody ~ poly(SBP, 4),
              data = train_df)

anova(poly_fit1, poly_fit2, poly_fit3, poly_fit4)
```
We can also test whether the polynomial terms are better for multiple predictors.

```{r}
# Compare linear vs. quadratic for multiple predictors
lm_linear <- lm(log_antibody ~ SBP + age, data = train_df)
lm_quad   <- lm(log_antibody ~ poly(SBP, 2) + poly(age, 2), data = train_df)

anova(lm_linear, lm_quad)  # Does adding quadratic terms help?
```
### Step Function

Using a step function model will create cuts in the data and fit models separately.It creates dummy variables for each region where the data is cut. You must add all the predictors individually and choose separately for each.

```{r}
step_fit = train(log_antibody ~ cut(SBP, 4),
              data = train_df,
              method = "lm")
```

### Cubic Splines

Cubic Splines fits a cubic polynomial piece wise model that has smooth transitions at the knots.

```{r}
cs_fit1 = train(log_antibody ~ bs(SBP, df = 4), # specify 4 knots
              data = train_df,
              method = "lm")

cs_fit2 = train(log_antibody ~ bs(SBP, knots = c(110, 120,130,140,150)), # specify values for knots
              data = train_df,
              method = "lm")
```

### Natural Cubic Splines

### Smoothing splines

# MSPEs

```{r}

MSPE = tibble(
  lm = MSPE_lm,
  ridge = MSPE_ridge,
  lasso = MSPE_lasso,
  enet = MSPE_enet,
  pcr = MSPE_pcr,
  pls = MSPE_pls,
  gam = MSPE_gam,
  mars = MSPE_mars
)

MSPE

```


